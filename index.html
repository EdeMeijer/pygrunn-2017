<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Deep learning with TensorFlow</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css">
    <link rel="stylesheet" href="css/custom.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <script src="js/jquery-3.2.1.js"></script>
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section data-background-color="#212121">
            <h1>
                Deep learning with<br>
                <img class="plain" src="img/logo.png">
            </h1>
            <aside class="notes">
                <strong>Summary</strong>
                <ul>
                    <li>Deep learning = machine learning</li>
                    <li>What is machine learning?</li>
                    <li>What is deep learning?</li>
                    <li>How training works</li>
                    <li>What is TensorFlow + how it works</li>
                    <li>Generating natural text</li>
                </ul>
            </aside>
        </section>
        <section>
            <section>
                <h2>Machine learning</h2>
                <h3>
                    Learning from data without being<br>explicitly programmed
                </h3>
                <aside class="notes">
                    The only thing we program is a framework that <strong>enables the computer to learn</strong>,
                    we don't build the actual model ourselves
                </aside>
                <div>
                    <ul>
                        <li>Linear regression</li>
                        <li>Logistic regression</li>
                        <li>Decision trees</li>
                        <li>Artificial neural networks</li>
                        <li>And much more...</li>
                    </ul>
                </div>
            </section>
            <section id="linear-regression-slide">
                <h2>Linear regression</h2>
                <p>
                    Predicting continuous values with linear weights<br>
                </p>
                $$Y = a + bX$$
                <aside class="notes">
                    Can only learn linear models
                </aside>
                <span class="diagram" id="linear-regression-diagram">
                    <span class="lr-x">Hot</span>
                    <span class="lr-y">Crazy</span>
                </span>
            </section>
            <section id="logistic-regression-slide">
                <h2>Logistic regression</h2>
                <p>
                    Classification with linear weights
                </p>
                <span class="diagram" id="logistic-regression-diagram"></span>
                <span class="label"><span class="dot orange-dot"></span>Orange dot</span>
                <span class="label"><span class="dot green-dot"></span>Green dot</span>
            </section>
            <section id="decision-trees-slide">
                <h2>Decision trees</h2>
                <p>
                    Predictions by splitting data in multiple steps
                </p>
                <p>
                    <small>What do people think of the weather?</small>
                </p>
                <div style="height: 490px" class="diagram" id="decision-trees-diagram">
                    <div id="dt-q1">
                        Raining?
                        <span class="yes">yes</span>
                        <span class="no">no</span>
                    </div>
                    <div class="bad" id="dt-r1">Bad</div>
                    <div id="dt-q2">
                        Temp > -10&#8451;?
                        <span class="yes">yes</span>
                        <span class="no">no</span>
                    </div>
                    <div class="bad" id="dt-r2">Bad</div>
                    <div id="dt-q3">
                        Snowing?
                        <span class="yes">yes</span>
                        <span class="no">no</span>
                    </div>
                    <div class="good" id="dt-r3">Good</div>
                    <div id="dt-q4">
                        Temp > 15&#8451;?
                        <span class="yes">yes</span>
                        <span class="no">no</span>
                    </div>
                    <div class="bad" id="dt-r4">Bad</div>
                    <div class="good" id="dt-r5">Good</div>
                </div>
            </section>
            <section id="ann-slide">
                <h2>Artificial neural networks</h2>
                <p>
                    Multi layered networks that can learn <strong class="tensorflow">nonlinear</strong> functions
                </p>
                <p>
                    <span class="diagram" id="ann-diagram"></span>
                    <span class="dn-diagram-in">Input layer</span>
                    <span class="dn-diagram-hidden">Hidden layer</span>
                    <span class="dn-diagram-out">Output layer</span>
                </p>
            </section>
        </section>
        <section data-background-color="#000000">
            <h2>Deep learning</h2>
            <section>
                <h3>
                    Artificial neural networks with many layers
                </h3>
                <p id="deep-network-diagram-fragment" class="fragment">
                    <span class="diagram" id="deep-network-diagram"></span>
                    <span class="dn-diagram-in">Input layer</span>
                    <span class="dn-diagram-hidden">Hidden layers</span>
                    <span class="dn-diagram-out">Output layer</span>
                </p>
            </section>
            <section>
                <h3>Features at multiple levels of abstraction</h3>
                <p>
                    <img src="img/deeplearning.png" style="width:80%">
                </p>
            </section>
            <section>
                <h3>Use cases include</h3>
                <ul>
                    <li>Speech recognition</li>
                    <li>Image recognition</li>
                    <li>Image captioning</li>
                    <li>
                        Natural language processing
                        <aside class="notes">NLP: Translation, sentiment analysis</aside>
                    </li>
                    <li>Recommendation systems</li>
                    <li><strong>Text generation</strong></li>
                </ul>
            </section>
        </section>
        <section id="gd-slide">
            <h2>Training a model</h2>
            <h3>Minimizing error with gradient descent</h3>
            <p>
                <small>Gradient descent follows the steepest decline along the error surface</small>
            </p>
            <aside class="notes">
                <ul>
                    <li>A gradient is the slope of the error surface</li>
                    <li>Step size is controlled by the learning rate</li>
                </ul>
            </aside>
            <p class="fragment" id="gd-fragment">
                <span class="diagram" id="gd-diagram">
                    <span class="lr-x" style="left: 135px;">Intercept</span>
                    <span class="lr-y" style="left: -30px;">Slope</span>
                    <span class="lr-x" style="left: 670px;">Hot</span>
                    <span class="lr-y" style="left: 360px;">Crazy</span>
                </span>
            </p>
        </section>
        <section data-background-color="#1D0A04">
            <h2>What is TensorFlow?</h2>
            <blockquote>"An open-source software library for Machine Intelligence"</blockquote>
            <div class="fragment">
                <p>Or Google's attempt to take over the world of A.I.</p>
                <small>It seems to be working out</small>
            </div>
        </section>
        <section>
            <h2>Tensors</h2>
            <h3>Arrays with any number of dimensions</h3>
            <ul>
                <li>A 0-D tensor is called a <strong>scalar</strong></li>
                <li>A 1-D tensor is called a <strong>vector</strong></li>
                <li>A 2-D tensor is called a <strong>matrix</strong></li>
                <li>An N-D tensor is called.. just a tensor</li>
            </ul>
        </section>
        <section>
            <h2>Computation graphs</h2>
            <p>
                Every calculation (operation) in TensorFlow is a <strong>node</strong><br>
            </p>
            <p>
                Nodes are connected by edges, here the tensors flows between operations.
            </p>
            <hr>
            $$Y = a + bX$$
            <hr>
            <img class="plain" src="img/graph.png" style="width: 500px;">
        </section>
        <section>
            <h2>Linear regression example</h2>
            <section>
                <p>
                <h3 style="position: absolute; left: 250px; top: 0">Code</h3>
                <h3 style="position: absolute; left: 760px; top: 0">Output</h3>
                <pre style="position: absolute; left: 0; top: 50px; width: 640px;"><code data-trim class="python">
import tensorflow as tf

# Define trainable variables
intercept = tf.Variable(2.0)
slope = tf.Variable(-1.0)

# Data intercept = 1.0, slope = 0.5
X = tf.constant([[0.0], [1.0], [2.0], [3.0]])
Y = tf.constant([[1.0], [1.5], [2.0], [2.5]])

# Calculate the prediction
prediction = X * slope + intercept

# Run the prediction and output the result
sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run(prediction))
                    </code></pre>
                <pre style="position: absolute; right: 0; top: 50px; width: 340px;"><code class="python">[[ 2.]
 [ 1.]
 [ 0.]
 [-1.]]
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    </code></pre>
                </p>
            </section>
            <section>
                <p>
                <h3 style="position: absolute; left: 250px; top: 0">Code</h3>
                <h3 style="position: absolute; left: 760px; top: 0">Output</h3>
                <pre style="position: absolute; left: 0; top: 50px; width: 640px;"><code data-trim data-noescape
                                                                                         class="python">
# Define trainable variables
intercept = tf.Variable(2.0)
slope = tf.Variable(-1.0)

# Data intercept = 1.0, slope = 0.5
X = tf.constant([[0.0], [1.0], [2.0], [3.0]])
Y = tf.constant([[1.0], [1.5], [2.0], [2.5]])

# Calculate the prediction and it's error
prediction = X * slope + intercept
<mark>squared_errors = tf.square(Y - prediction)
mse = tf.reduce_mean(squared_errors)</mark>

# Run the prediction and output the result
sess = tf.Session()
sess.run(tf.global_variables_initializer())
<mark>print(sess.run(squared_errors))
print(sess.run(mse))</mark>
                    </code></pre>
                <pre style="position: absolute; right: 0; top: 50px; width: 340px;"><code class="python">[[  1.  ]
 [  0.25]
 [  4.  ]
 [ 12.25]]
4.375
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    </code></pre>
                </p>
            </section>
            <section>
                <p>
                <h3 style="position: absolute; left: 250px; top: 0">Code</h3>
                <h3 style="position: absolute; left: 760px; top: 0">Output</h3>
                <pre style="position: absolute; left: 0; top: 50px; width: 640px;"><code data-trim data-noescape
                                                                                         class="python">
# Calculate the prediction and it's error
prediction = X * slope + intercept
squared_errors = tf.square(Y - prediction)
mse = tf.reduce_mean(squared_errors)

# Define the gradient descent operation
<mark>train_op = tf.train.GradientDescentOptimizer(
    learning_rate=0.1).minimize(mse)</mark>

# Run the prediction and output the result
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for i in range(100): # Optimize for 100 steps
    _, a, b, err = sess.run([
        <mark>train_op</mark>, intercept, slope, mse])
    if i % 10 == 0:
        print('{}  {:.2f}  {:.2f}  {:.2f}'
            .format(i, a, b, err))
                    </code></pre>
                <pre style="position: absolute; right: 0; top: 50px; width: 340px;"><code class="python">0  2.00  -1.00  4.38
10  1.76  0.15  0.21
20  1.41  0.31  0.06
30  1.22  0.40  0.02
40  1.12  0.44  0.01
50  1.07  0.47  0.00
60  1.04  0.48  0.00
70  1.02  0.49  0.00
80  1.01  0.50  0.00
90  1.01  0.50  0.00
                    
                    
                    
                    
                    
                    
                    
                    
                    </code></pre>
                </p>
            </section>
        </section>
        <section data-background-image="img/pygrunn.png">
            <h1 style="color: white; text-shadow: 0 0 10px black">Deep Learning in TensorFlow</h1>
            <h2 style="color: #ff8257 !important; text-shadow: 0 0 10px black">Let's build this Trump robot!</h2>
        </section>
        <section>
            <h2>Deep Trump tweet generator</h2>
            <p>
                Over <strong>30.000</strong> tweets since 2009<br>
                About 20.000 usable tweets (no retweets etc)
            </p>
            <img class="plain" src="img/tweet.png">
        </section>
        <section>
            <h2>Recurrent neural network model</h2>
            <img class="plain" style="float:left; height: 600px; margin: 0 20px 0 0" src="img/model.png">
            <section style="right: 0; width: 800px;">
                <h3>Acts on multiple time steps</h3>
            </section>
            <section style="right: 0; width: 800px;">
                <p><strong>LSTM</strong> (Long Short-Term Memory) cells can remember state over many time steps</p>
                <img class="plain" src="img/lstm.png" style="width:600px;">
                <p>
                    In our case, it will learn to remember the context of the current word and sentence
                </p>
            </section>
            <section style="right: 0; width: 800px;">
                <p><strong>ReLU</strong> (Rectified Linear Unit) adds more <strong>nonlinearity</strong></p>
                <img class="plain" src="img/relu.png" style="width:300px;">
                <p>
                    It will give the network greater modelling power, improving prediction quality
                </p>
            </section>
            <section style="right: 0; width: 800px;">
                <p><strong>Softmax</strong> squashes the probabilities of all outputs to a sum of 1.0</p>
                <img class="plain" src="img/softmax.png" style="width:600px;">
                <p>
                    This allows us to sample characters from a probability distribution when generating tweets
                </p>
            </section>
        </section>
        <section>
            <h2>Data format</h2>
            <table class="data-ex">
                <tr><td colspan="13"><strong>Features</strong> (input data)</tr>
                <tr class="real">
                    <td>⇒</td>
                    <td>#</td>
                    <td>T</td>
                    <td>r</td>
                    <td>u</td>
                    <td>m</td>
                    <td>p</td>
                    <td>2</td>
                    <td>0</td>
                    <td>1</td>
                    <td>6</td>
                    <td></td>
                    <td></td>
                </tr>
                <tr class="tensorflow">
                    <td>81</td>
                    <td>4</td>
                    <td>46</td>
                    <td>71</td>
                    <td>74</td>
                    <td>66</td>
                    <td>69</td>
                    <td>16</td>
                    <td>14</td>
                    <td>15</td>
                    <td>20</td>
                    <td></td>
                    <td></td>
                </tr>
                <tr><td colspan="13"><strong>Labels</strong> (output data)</td></tr>
                <tr class="real">
                    <td>#</td>
                    <td>T</td>
                    <td>r</td>
                    <td>u</td>
                    <td>m</td>
                    <td>p</td>
                    <td>2</td>
                    <td>0</td>
                    <td>1</td>
                    <td>6</td>
                    <td>⛔</td>
                    <td></td>
                    <td></td>
                </tr>
                <tr class="green">
                    <td>4</td>
                    <td>46</td>
                    <td>71</td>
                    <td>74</td>
                    <td>66</td>
                    <td>69</td>
                    <td>16</td>
                    <td>14</td>
                    <td>15</td>
                    <td>20</td>
                    <td>81</td>
                    <td></td>
                    <td></td>
                </tr>
                <tr><td colspan="13"><strong>Mask</strong></td></tr>
                <tr class="blue">
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>0</td>
                    <td>0</td>
                </tr>
            </table>
        </section>
        <section class="full-code">
            <h2>TensorFlow implementation</h2>
            <section>
                <small>data_set.py</small>
                <pre><code data-trim class="python scroll4">
import os
import numpy as np

CACHE_DIR = os.path.dirname(os.path.abspath(__file__)) + '/cache'

def load():
    return DataSet(
        np.load(CACHE_DIR + '/features.npy'),
        np.load(CACHE_DIR + '/labels.npy'),
        np.load(CACHE_DIR + '/mask.npy')
    )

class DataSet:
    def __init__(self, features, labels, mask):
        self.features = features
        self.labels = labels
        self.mask = mask

    def split_test_train(self, test_ratio=0.1):
        n_test = round(self.features.shape[0] * test_ratio)

        test = DataSet(self.features[:n_test], self.labels[:n_test], self.mask[:n_test])
        train = DataSet(self.features[n_test:], self.labels[n_test:], self.mask[n_test:])

        return test, train
                </code></pre>
            </section>
            <section>
                <small>model.py</small>
                <pre><code data-trim class="python scroll4">
import tensorflow as tf

def make_weight_variable(name, num_inputs, num_outputs):
    return tf.get_variable(
        name,
        [num_inputs, num_outputs],
        initializer=tf.contrib.layers.variance_scaling_initializer()
    )

class Model:
    def __init__(self, chars, max_steps, lstm_units=250, l1_units=200, l2_units=150,
                 learning_rate=0.001, l2=0.001):
        self.chars = chars
        self.max_steps = max_steps

        # Define placeholders for training data
        self.features = tf.placeholder(dtype=tf.int32, shape=[None, max_steps])
        self.labels = tf.placeholder(dtype=tf.int32, shape=[None, max_steps])
        self.mask = tf.placeholder(dtype=tf.float32, shape=[None, max_steps])

        # Define LSTM layer
        features_one_hot = tf.one_hot(self.features, len(chars) + 1, dtype=tf.float32)

        lstm_3d, _ = tf.nn.dynamic_rnn(
            cell=tf.contrib.rnn.LSTMCell(num_units=lstm_units),
            dtype=tf.float32,
            inputs=features_one_hot
        )
        lstm_flat = tf.reshape(lstm_3d, [-1, lstm_units])

        # Define first ReLU layer
        l1_weights = make_weight_variable("l1-weights", lstm_units, l1_units)
        l1_biases = tf.Variable(0.1, name='l1-biases')
        layer1 = tf.nn.relu(tf.matmul(lstm_flat, l1_weights) + l1_biases)

        # Define second ReLU layer
        l2_weights = make_weight_variable("l2-weights", l1_units, l2_units)
        l2_biases = tf.Variable(0.1, name='l2-biases')
        layer2 = tf.nn.relu(tf.matmul(layer1, l2_weights) + l2_biases)

        # Define output layer
        out_weights = make_weight_variable("out-weights", l2_units, len(chars) + 1)
        out_biases = tf.Variable(0.1, name='out-biases')
        self.out_logits = tf.matmul(layer2, out_weights) + out_biases

        # Define training objective
        loss_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(
            labels=tf.reshape(self.labels, [-1]),
            logits=self.out_logits
        )
        loss_flat_masked = loss_flat * tf.reshape(self.mask, [-1])
        self.loss = tf.reduce_mean(loss_flat_masked)

        weight_vars = [v for v in tf.trainable_variables() if 'bias' not in v.name]
        self.l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in weight_vars]) * l2

        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        self.train_op = optimizer.minimize(self.loss + self.l2_loss)

                </code></pre>
            </section>
            <section>
                <small>tweet_sampler.py</small>
                <pre><code data-trim class="python scroll4">
import random

import numpy as np
import tensorflow as tf

MAX_SAMPLE_LENGTH = 200

class TweetSampler:
    def __init__(self, session, model, temperature=1.0):
        self.session = session
        self.model = model
        self.predictions_flat = tf.nn.softmax(model.out_logits / temperature, 1)

    def sample(self):
        # Start with the start symbol, which has label num_chars
        features = [len(self.model.chars)]
        tweet = ''

        for i in range(MAX_SAMPLE_LENGTH):
            next_class = self.sample_next_class(features[-self.model.max_steps:])
            if next_class == len(self.model.chars):
                break
            features.append(next_class)
            tweet += self.model.chars[next_class]

        return tweet.strip()

    def sample_next_class(self, classes):
        sample_input = np.zeros([1, self.model.max_steps])
        sample_input[:, :len(classes)] = classes

        predictions = self.session.run(
            self.predictions_flat,
            feed_dict={
                self.model.features: sample_input,
            }
        )
        probabilities = predictions[len(classes) - 1]
        rnd = random.random()
        accum = 0
        for idx in range(len(probabilities)):
            accum += probabilities[idx]
            if accum >= rnd:
                return idx
        return np.argmax(classes)
                </code></pre>
            </section>
            <section>
                <small>train.py</small>
                <pre><code data-trim class="python scroll4">
import json
import math
import os

import tensorflow as tf

import data_set
from model import Model
from tweet_sampler import TweetSampler

CACHE_DIR = os.path.dirname(os.path.abspath(__file__)) + '/cache'
BATCH_SIZE = 64
NUM_EPOCHS = 200

test_data, train_data = data_set.load().split_test_train()

with open(CACHE_DIR + '/settings.json') as file:
    settings = json.load(file)

model = Model(
    settings['chars'],
    settings['maxSteps'],
    lstm_units=500,
    l1_units=400,
    l2_units=300,
    l2=0.00005
)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

sampler = TweetSampler(sess, model, temperature=0.8)
                    
def output_tweet_sample():
    print('Sampling tweet....\n\n---------------------------------------')
    print(sampler.sample())
    print('---------------------------------------\n')

def process_data(data, ops):
    num_examples = data.features.shape[0]
    num_batches = math.ceil(num_examples / BATCH_SIZE)
    for batch in range(num_batches):
        start = batch * BATCH_SIZE
        end = (batch + 1) * BATCH_SIZE
        yield sess.run(
            ops,
            feed_dict={
                model.features: data.features[start:end],
                model.labels: data.labels[start:end],
                model.mask: data.mask[start:end]
            }
        )

def calc_test_error():
    total_err = 0
    num_batches = 0
    for err in process_data(test_data, ops=model.loss):
        total_err += err
        num_batches += 1
    return total_err / num_batches
                    
def train_epoch(epoch):
    num_batches = 0
    total_err = 0
    total_l2 = 0

    ops = [model.train_op, model.loss, model.l2_loss]
    for _, err, l2 in process_data(train_data, ops=ops):
        total_err += err
        total_l2 += l2
        num_batches += 1

    print('EPOCH {}: train = {:.5}, test = {:.5}, L2 = {:.5}'.format(
        epoch, total_err / num_batches, calc_test_error(), total_l2 / num_batches))
    output_tweet_sample()

saver = tf.train.Saver(max_to_keep=NUM_EPOCHS)
output_tweet_sample()
for epoch in range(0, NUM_EPOCHS):
    train_epoch(epoch)
    save_path = saver.save(sess, CACHE_DIR + "/model/model.ckpt", global_step=epoch)
    print("Model saved in file: %s" % save_path)
                </code></pre>
            </section>
        </section>
        <section>
            <h2>Running the training code</h2>
            <pre class="console">
$ python3 train.py
Sampling tweet....

---------------------------------------
<strong>W6upDK7rEFkkMu?Xno5wT)Wr'XIG'zWNb$E6njei0</strong>
---------------------------------------

EPOCH 0: train = 2.1826, test = 1.8535, L2 = 0.065104
Sampling tweet....

---------------------------------------
<strong>Tomup's shem beade s Pand Preckile Ammanar Ifain in the Obamay. The soorso I-Grat Liad is ack is an groming chat go with Amp remelas wor poom bf the proutthoul Mrane. I- O FRTET GE that I dond boing!</strong>
---------------------------------------

Model saved in file: /home/ede/repos/trumpet/cache/model/model.ckpt-0
EPOCH 1: train = 1.6961, test = 1.6108, L2 = 0.082404
Sampling tweet....

---------------------------------------
<strong>The wascon to the workn of the   Whind fould monned - lost just that mysices for com.).</strong>
---------------------------------------

Model saved in file: /home/ede/repos/trumpet/cache/model/model.ckpt-1
EPOCH 2: train = 1.5106, test = 1.4638, L2 = 0.093441
Sampling tweet....

---------------------------------------
<strong>Wasting a dick and of the BOfler Bartor conting support of toums. Fom Hampshers. Koppord w/Ther thann. I will See Happer. Big is smailes are, problems. America, the pasiques coulst in I wanded and par</strong>
---------------------------------------
            </pre>
        </section>
        <section>
            <h2>After more epochs it gets better :)</h2>
            <pre class="console">
EPOCH 81: train = 0.80645, test = 1.2222, L2 = 0.2602
Sampling tweet....

---------------------------------------
<strong>"@Patrick I through my news a better action they'vo know how to play the shirt too let the players and killing thousands of job!"</strong>
---------------------------------------

Model saved in file: /home/ede/repos/trumpet/cache/model/model.ckpt-81
EPOCH 82: train = 0.80586, test = 1.227, L2 = 0.26108
Sampling tweet....

---------------------------------------
<strong>"@erinbrinz: @foxandfriends: Amnesty Can Meason Exastele 'Globe thought's one on iP. You have easy still even planned in order."</strong>
---------------------------------------

Model saved in file: /home/ede/repos/trumpet/cache/model/model.ckpt-82
EPOCH 83: train = 0.80409, test = 1.2229, L2 = 0.2618
Sampling tweet....

---------------------------------------
<strong>@toomlyinl @bringinsfording @bottomert @BarackObama keep Democrab, the government @BarackObama send three honest http://t.co/HyYNOxFS50"</strong>
---------------------------------------
            </pre>
        </section>
        <section>
            <h2>Success!</h2>
            <img class="plain" style="width: 600px" src="img/success.png">
            <p class="green">All code is on Github!</p>
            <table>
                <tr>
                    <th>Trumpet</th>
                    <td>https://github.com/EdeMeijer/trumpet</td>
                </tr>
                <tr>
                    <th>Slides</th>
                    <td>https://github.com/EdeMeijer/pygrunn-2017</td>
                </tr>
            </table>
            <p class="left">
                <small> ede@buybrain.io</small>
            </p>
        </section>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/chroma.min.js"></script>
<script src="js/reveal.js"></script>

<script>
    Reveal.initialize({
        width: 1024,
        height: 768,
        math: {
            mathjax: 'js/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },
        dependencies: [
            {src: 'plugin/markdown/marked.js'},
            {src: 'plugin/markdown/markdown.js'},
            {src: 'plugin/notes/notes.js', async: true},
            {src: 'plugin/math/math.js', async: true},
            {
                src: 'plugin/highlight/highlight.js',
                async: true,
                callback: function () {
                    hljs.initHighlightingOnLoad();
                }
            }
        ]
    });

    Reveal.addEventListener('slidechanged', evt => {
        $(evt.currentSlide).trigger('show');
        $(evt.previousSlide).trigger('hide');
    });

    Reveal.addEventListener('fragmentshown', evt => {
        $(evt.fragment).trigger('show');
    });

    Reveal.addEventListener('fragmenthidden', evt => {
        $(evt.fragment).trigger('hide');
    });

    $(() => setTimeout(
        () => $(Reveal.getCurrentSlide()).trigger('show'),
        10
    ));
</script>
<script src="https://cdn.jsdelivr.net/mojs/0.265.6/mo.min.js"></script>
<script src="js/anim.js"></script>
</body>
</html>
